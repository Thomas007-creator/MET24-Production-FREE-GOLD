// ü§ñ all-MiniLM-L12-v2 Embedding Service for MET24
// Privacy-first local embedding generation with high quality for MBTI coaching

// import { pipeline, env } from '@xenova/transformers'; // Temporarily disabled for deployment

interface EmbeddingResult {
  embedding: number[];
  inferenceTime: number;
  dimensions: number;
  model: string;
}

interface BatchEmbeddingResult {
  embeddings: number[][];
  totalInferenceTime: number;
  averageInferenceTime: number;
}

interface ModelInfo {
  name: string;
  dimensions: number;
  description: string;
  provider: string;
  privacy: string;
  languages: string[];
  optimizedFor: string[];
}

class MPNetL12EmbeddingService {
  private model: any = null;
  private readonly modelName = 'Xenova/all-MiniLM-L12-v2';
  private readonly dimensions = 384;
  private isInitialized = false;
  private initializationPromise: Promise<void> | null = null;
  
  async initialize(): Promise<void> {
    if (this.isInitialized) return;
    
    if (this.initializationPromise) {
      return this.initializationPromise;
    }
    
    this.initializationPromise = this._doInitialize();
    return this.initializationPromise;
  }
  
  private async _doInitialize(): Promise<void> {
    try {
      console.log('üöÄ Initializing all-MiniLM-L12-v2 model...');
      
      // Temporarily disabled for deployment - AI/ML features will be re-enabled later
      throw new Error('AI/ML features temporarily disabled for deployment');
      
    } catch (error) {
      console.error('‚ùå Failed to initialize all-MiniLM-L12-v2:', error);
      this.initializationPromise = null;
      throw error;
    }
  }
  
  async generateEmbedding(text: string): Promise<EmbeddingResult> {
    await this.initialize();
    
    const startTime = Date.now();
    
    try {
      // Generate 384-dimensional embedding with mean pooling and normalization
      const result = await this.model(text, { 
        pooling: 'mean', 
        normalize: true 
      });
      
      const embedding = Array.from(result.data) as number[];
      const inferenceTime = Date.now() - startTime;
      
      // Validate dimensions
      if (embedding.length !== this.dimensions) {
        throw new Error(`Expected ${this.dimensions} dimensions, got ${embedding.length}`);
      }
      
      return {
        embedding,
        inferenceTime,
        dimensions: this.dimensions,
        model: 'all-MiniLM-L12-v2'
      };
      
    } catch (error) {
      console.error('‚ùå Embedding generation failed:', error);
      throw new Error(`Failed to generate embedding: ${error instanceof Error ? error.message : String(error)}`);
    }
  }
  
  async generateBatchEmbeddings(texts: string[]): Promise<BatchEmbeddingResult> {
    await this.initialize();
    
    const startTime = Date.now();
    const embeddings: number[][] = [];
    
    for (const text of texts) {
      const result = await this.generateEmbedding(text);
      embeddings.push(result.embedding);
    }
    
    const totalInferenceTime = Date.now() - startTime;
    const averageInferenceTime = totalInferenceTime / texts.length;
    
    return {
      embeddings,
      totalInferenceTime,
      averageInferenceTime
    };
  }
  
  cosineSimilarity(a: number[], b: number[]): number {
    if (a.length !== b.length) {
      throw new Error('Vectors must have same dimensions');
    }
    
    const dotProduct = a.reduce((sum, val, i) => sum + val * b[i], 0);
    const magnitudeA = Math.sqrt(a.reduce((sum, val) => sum + val * val, 0));
    const magnitudeB = Math.sqrt(b.reduce((sum, val) => sum + val * val, 0));
    
    return dotProduct / (magnitudeA * magnitudeB);
  }
  
  async findSimilarContent(
    queryText: string,
    contentEmbeddings: Array<{id: string; embedding: number[]; content: string}>
  ): Promise<Array<{id: string; similarity: number; content: string}>> {
    const queryResult = await this.generateEmbedding(queryText);
    
    return contentEmbeddings
      .map(item => ({
        id: item.id,
        similarity: this.cosineSimilarity(queryResult.embedding, item.embedding),
        content: item.content
      }))
      .sort((a, b) => b.similarity - a.similarity);
  }
  
  getModelInfo(): ModelInfo {
    return {
      name: 'all-MiniLM-L12-v2',
      dimensions: this.dimensions,
      description: 'High-quality sentence embeddings for MBTI coaching',
      provider: 'Microsoft/Sentence-Transformers',
      privacy: 'Local processing only',
      languages: ['English', 'Dutch', 'Multilingual'],
      optimizedFor: ['Semantic search', 'MBTI personality analysis', 'Mental health content']
    };
  }
  
  // üéØ MBTI-specific embedding helpers
  async generatePersonalityEmbedding(personalityData: {
    mbtiType?: string;
    traits?: string[];
    challenges?: string[];
    goals?: string[];
  }): Promise<EmbeddingResult> {
    const personalityText = [
      personalityData.mbtiType || '',
      ...(personalityData.traits || []),
      ...(personalityData.challenges || []),
      ...(personalityData.goals || [])
    ].filter(Boolean).join(' ');
    
    return this.generateEmbedding(personalityText);
  }
  
  async generateCoachingContextEmbedding(context: {
    userMessage: string;
    previousContext?: string;
    emotionalState?: string;
    sessionGoals?: string[];
  }): Promise<EmbeddingResult> {
    const contextText = [
      context.userMessage,
      context.previousContext || '',
      context.emotionalState || '',
      ...(context.sessionGoals || [])
    ].filter(Boolean).join(' ');
    
    return this.generateEmbedding(contextText);
  }
  
  // Performance monitoring
  async runPerformanceTest(sampleTexts: string[]): Promise<{
    averageInferenceTime: number;
    totalTime: number;
    throughput: number;
    modelInfo: ModelInfo;
  }> {
    console.log('üß™ Running all-MiniLM-L12-v2 performance test...');
    
    const batchResult = await this.generateBatchEmbeddings(sampleTexts);
    const throughput = sampleTexts.length / (batchResult.totalInferenceTime / 1000); // texts per second
    
    return {
      averageInferenceTime: batchResult.averageInferenceTime,
      totalTime: batchResult.totalInferenceTime,
      throughput,
      modelInfo: this.getModelInfo()
    };
  }
}

// Singleton instance
export const mpnetL12EmbeddingService = new MPNetL12EmbeddingService();

// Export types for TypeScript support
export type { EmbeddingResult, BatchEmbeddingResult, ModelInfo };
export default mpnetL12EmbeddingService;